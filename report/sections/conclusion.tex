\section{Conclusion}
This study offers a comprehensive evaluation of multiple \textbf{clustering algorithms} under various \textbf{hyperparameter} settings across three distinct \textbf{datasets}. \textbf{Global K-Means} provides more stable and consistent partitions than standard \textbf{K-Means}, while \textbf{FCM} adapts effectively to heterogeneous density and structure. \textbf{X-Means} proves valuable for datasets with unclear cluster counts, dynamically refining cluster numbers as needed.

\textbf{Euclidean distance} generally outperforms other metrics, especially in higher-dimensional scenarios. \textbf{Data characteristics} strongly influence optimal parameters: lower \textbf{k} suits the binary \textbf{Hepatitis} data, while intermediate or higher \textbf{k} values better capture the complexity of \textbf{Pen-based} data. \textbf{Dimensionality reduction} techniques like \textbf{PCA} and \textbf{UMAP} enhance interpretability, aiding in selecting and refining methods.

Ultimately, careful \textbf{tuning} of \textbf{clustering algorithms}, informed by \textbf{data attributes} and proper \textbf{evaluation metrics}, is crucial for uncovering meaningful structures in diverse datasets.